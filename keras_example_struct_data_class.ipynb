{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本教程展示了如何对结构化数据进行分类（例如CSV中的表格数据）。我们使用Keras定义模型，并将csv中各列的特征转化为训练的输入。 本教程包含一下功能代码：\n",
    "\n",
    "# 使用Pandas加载CSV文件。\n",
    "# 构建一个输入的pipeline，使用tf.data批处理和打乱数据。\n",
    "# 从CSV中的列映射到用于训练模型的输入要素。\n",
    "# 使用Keras构建，训练和评估模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1500)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.数据集\n",
    "# 我们将使用克利夫兰诊所心脏病基金会提供的一个小数据集。 CSV中有几百行。 每行描述一个患者，每列描述一个属性。 我们将使用此信息来预测患者是否患有心脏病，该疾病在该数据集中是二元分类任务。\n",
    "\n",
    "# Column\tDescription\tFeature Type\tData Type\n",
    "# Age\tAge in years\tNumerical\tinteger\n",
    "# Sex\t(1 = male; 0 = female)\tCategorical\tinteger\n",
    "# CP\tChest pain type (0, 1, 2, 3, 4)\tCategorical\tinteger\n",
    "# Trestbpd\tResting blood pressure (in mm Hg on admission to the hospital)\tNumerical\tinteger\n",
    "# Chol\tSerum cholestoral in mg/dl\tNumerical\tinteger\n",
    "# FBS\t(fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\tCategorical\tinteger\n",
    "# RestECG\tResting electrocardiographic results (0, 1, 2)\tCategorical\tinteger\n",
    "# Thalach\tMaximum heart rate achieved\tNumerical\tinteger\n",
    "# Exang\tExercise induced angina (1 = yes; 0 = no)\tCategorical\tinteger\n",
    "# Oldpeak\tST depression induced by exercise relative to rest\tNumerical\tinteger\n",
    "# Slope\tThe slope of the peak exercise ST segment\tNumerical\tfloat\n",
    "# CA\tNumber of major vessels (0-3) colored by flourosopy\tNumerical\tinteger\n",
    "# Thal\t3 = normal; 6 = fixed defect; 7 = reversable defect\tCategorical\tstring\n",
    "# Target\tDiagnosis of heart disease (1 = true; 0 = false)\tClassification\tinteger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.准备数据\n",
    "# 使用pandas读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>reversible</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   1       145   233    1        2      150      0      2.3      3   \n",
       "1   67    1   4       160   286    0        2      108      1      1.5      2   \n",
       "2   67    1   4       120   229    0        2      129      1      2.6      2   \n",
       "3   37    1   3       130   250    0        0      187      0      3.5      3   \n",
       "4   41    0   2       130   204    0        2      172      0      1.4      1   \n",
       "\n",
       "   ca        thal  target  \n",
       "0   0       fixed       0  \n",
       "1   3      normal       1  \n",
       "2   2  reversible       0  \n",
       "3   0      normal       0  \n",
       "4   0      normal       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL = 'https://storage.googleapis.com/applied-dl/heart.csv'\n",
    "dataframe = pd.read_csv('heart.csv')\n",
    "dataframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集验证集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193 train examples\n",
      "49 validation examples\n",
      "61 test examples\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(dataframe, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用tf.data构造输入pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop('target')\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds\n",
    "\n",
    "batch_size = 5\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature: ['thal', 'age', 'exang', 'fbs', 'slope', 'sex', 'thalach', 'cp', 'oldpeak', 'chol', 'restecg', 'trestbps', 'ca']\n",
      "A batch of ages: tf.Tensor([54 62 44 64 67], shape=(5,), dtype=int32)\n",
      "A batch of targets: tf.Tensor([1 0 1 1 1], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "    print('Every feature:', list(feature_batch.keys()))\n",
    "    print('A batch of ages:', feature_batch['age'])\n",
    "    print('A batch of targets:', label_batch )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.tensorflow的feature column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = next(iter(train_ds))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(feature_column):\n",
    "    feature_layer = layers.DenseFeatures(feature_column)\n",
    "    print(feature_layer(example_batch).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数字列\n",
    "# 特征列的输出成为模型的输入。 数字列是最简单的列类型。\n",
    "# 它用于表示真正有价值的特征。 使用此列时，模型将从数据框中接收未更改的列值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[54.]\n",
      " [62.]\n",
      " [44.]\n",
      " [64.]\n",
      " [67.]]\n"
     ]
    }
   ],
   "source": [
    "age = feature_column.numeric_column(\"age\")\n",
    "demo(age)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Bucketized列（桶列）\n",
    "# 通常，您不希望将数字直接输入模型，而是根据数值范围将其值分成不同的类别。 考虑代表一个人年龄的原始数据。 我们可以使用bucketized列将年龄分成几个桶，而不是将年龄表示为数字列。\n",
    "# 请注意，下面的one-hot描述了每行匹配的年龄范围\n",
    "age_buckets = feature_column.bucketized_column(age, boundaries=[\n",
    "    18, 25, 30, 35, 40, 50\n",
    "])\n",
    "demo(age_buckets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 类别列\n",
    "# 在该数据集中，thal表示为字符串（例如“固定”，“正常”或“可逆”）。 \n",
    "# 我们无法直接将字符串提供给模型。 相反，我们必须首先将它们映射到数值。\n",
    "# 类别列提供了一种将字符串表示为单热矢量的方法（就像上面用年龄段看到的那样）。\n",
    "# 类别表可以使用categorical_column_with_vocabulary_list作为列表传递，\n",
    "# 或者使用categorical_column_with_vocabulary_file从文件加载。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0715 07:23:06.222284 140227208578816 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:2655: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0715 07:23:06.229785 140227208578816 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:4215: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "W0715 07:23:06.231249 140227208578816 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:4270: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "thal = feature_column.categorical_column_with_vocabulary_list('thal', ['fixed', 'normal', 'reversible'])\n",
    "thal_one_hot = feature_column.indicator_column(thal)\n",
    "demo(thal_one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.18246703  0.00358586 -0.08059143 -0.16413437 -0.6673732   0.61640644\n",
      "   0.12320928  0.32749712]\n",
      " [ 0.18246703  0.00358586 -0.08059143 -0.16413437 -0.6673732   0.61640644\n",
      "   0.12320928  0.32749712]\n",
      " [ 0.18246703  0.00358586 -0.08059143 -0.16413437 -0.6673732   0.61640644\n",
      "   0.12320928  0.32749712]\n",
      " [-0.14413896  0.53273565  0.2131288  -0.15537444 -0.10157444 -0.19141938\n",
      "  -0.0374186  -0.22458434]\n",
      " [ 0.18246703  0.00358586 -0.08059143 -0.16413437 -0.6673732   0.61640644\n",
      "   0.12320928  0.32749712]]\n"
     ]
    }
   ],
   "source": [
    "# 嵌入列\n",
    "# 假设我们不是只有几个可能的字符串，而是每个类别有数千（或更多）值。\n",
    "# 由于多种原因，随着类别数量的增加，使用单热编码训练神经网络变得不可行。 \n",
    "# 我们可以使用嵌入列来克服此限制。\n",
    "# 嵌入列不是将数据表示为多维度的单热矢量，而是将数据表示为低维密集向量，\n",
    "# 其中每个单元格可以包含任意数字，而不仅仅是0或1.嵌入的大小是必须训练调整的参数。\n",
    "\n",
    "thal_embedding = feature_column.embedding_column(thal, dimension=8)\n",
    "demo(thal_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 哈希特征列\n",
    "# 表示具有大量值的分类列的另一种方法是使用categorical_column_with_hash_bucket。\n",
    "# 此功能列计算输入的哈希值，然后选择一个hash_bucket_size存储桶来编码字符串。\n",
    "# 使用此列时，您不需要提供词汇表，\n",
    "# 并且可以选择使hash_buckets的数量远远小于实际类别的数量以节省空间。\n",
    "\n",
    "# 注：该技术的一个重要缺点是可能存在冲突，其中不同的字符串被映射到同一个桶。\n",
    "# --------------------- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0715 07:23:07.716157 140227208578816 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:4270: HashedCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "thal_hashed = feature_column.categorical_column_with_hash_bucket('thal', hash_bucket_size=1000)\n",
    "demo(feature_column.indicator_column(thal_hashed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉功能列\n",
    "# 将特征组合成单个特征（更好地称为特征交叉），使模型能够为每个特征组合学习单独的权重。 \n",
    "# 在这里，我们将创建一个与age和thal交叉的新功能。 \n",
    "# 请注意，crossed_column不会构建所有可能组合的完整表（可能非常大）。 \n",
    "# 相反，它由hashed_column支持，因此您可以选择表的大小。\n",
    "# --------------------- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0715 07:23:08.508663 140227208578816 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:4270: CrossedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\n",
    "demo(feature_column.indicator_column(crossed_feature))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.选择使用feature column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "# numeric cols\n",
    "for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\n",
    "    feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "# bucketized cols\n",
    "age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "feature_columns.append(age_buckets)\n",
    "\n",
    "# indicator cols\n",
    "thal = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'thal', ['fixed', 'normal', 'reversible'])\n",
    "thal_one_hot = feature_column.indicator_column(thal)\n",
    "feature_columns.append(thal_one_hot)\n",
    "\n",
    "# embedding cols\n",
    "thal_embedding = feature_column.embedding_column(thal, dimension=8)\n",
    "feature_columns.append(thal_embedding)\n",
    "\n",
    "# crossed cols\n",
    "crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\n",
    "crossed_feature = feature_column.indicator_column(crossed_feature)\n",
    "feature_columns.append(crossed_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建特征层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.feature_column.feature_column_v2.DenseFeatures at 0x7f88114446a0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.构建模型并训练\n",
    "# layers.Dense(128, activation='relu'),\n",
    "# layers.Dropout(0.5),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable weights: 0\n",
      "trainable weights: 4\n"
     ]
    }
   ],
   "source": [
    "# # 配置只有训练时可以执行的网络层\n",
    "class MyDropout(layers.Layer):\n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super(MyDropout, self).__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "    def call(self, inputs, training=None):\n",
    "        return tf.cond(training, \n",
    "                       lambda: tf.nn.dropout(inputs, rate=self.rate),\n",
    "                      lambda: inputs)\n",
    "\n",
    "#2.使用子层递归构建网络层\n",
    "class MyBlock_dense_norm_relu_drop_relu(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyBlock_dense_norm_relu_drop_relu, self).__init__()\n",
    "        self.dense = layers.Dense(128)\n",
    "        self.batch_norm=layers.BatchNormalization()\n",
    "#         self.drop_out=layers.Dropout(0.5)\n",
    "        self.drop_out=MyDropout(0.5)\n",
    "        self.reul_func1=layers.Activation('relu')\n",
    "        self.reul_func2=layers.Activation('relu')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        h1 = self.dense(inputs)\n",
    "        h1 = self.batch_norm(h1) \n",
    "#         h1 = tf.nn.relu(h1)\n",
    "        h1=  self.reul_func1(h1)\n",
    "        h1=self.drop_out(h1)\n",
    "#         h1 = tf.nn.relu(h1)\n",
    "        h1=  self.reul_func2(h1)\n",
    "        return h1\n",
    "    \n",
    "my_block = MyBlock_dense_norm_relu_drop_relu()\n",
    "print('trainable weights:', len(my_block.trainable_weights))\n",
    "y = my_block(tf.ones(shape=(3, 64)))\n",
    "# 构建网络在build()里面，所以执行了才有网络\n",
    "print('trainable weights:', len(my_block.trainable_weights)) \n",
    "# print(my_block.losses) \n",
    "# print(my_block.weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自定义dense->batchnorm->relu->dropout->relu block结构\n",
    "model = tf.keras.Sequential([\n",
    "    feature_layer,\n",
    "    MyBlock_dense_norm_relu_drop_relu(),\n",
    "\n",
    "    MyBlock_dense_norm_relu_drop_relu(),\n",
    "\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##原始\n",
    "# model = tf.keras.Sequential([\n",
    "#     feature_layer,\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "# #     layers.BatchNormalization(),\n",
    "# #     layers.Dropout(0.2),\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "# #     layers.BatchNormalization(),\n",
    "# #     layers.Dropout(0.2),\n",
    "#     layers.Dense(1, activation='sigmoid')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自己加了batchnorm和dropout\n",
    "# model = tf.keras.Sequential([\n",
    "#     feature_layer,\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Dropout(0.5),\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Dropout(0.5),\n",
    "#     layers.Dense(1, activation='sigmoid')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(\n",
    "             optimizer='adam',\n",
    "#              optimizer=keras.optimizers.SGD(),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'],\n",
    "             run_eagerly=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0715 07:25:54.293545 140227208578816 ag_logging.py:145] Entity <bound method MyBlock_dense_norm_relu_drop_relu.call of <__main__.MyBlock_dense_norm_relu_drop_relu object at 0x7f8811421c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MyBlock_dense_norm_relu_drop_relu.call of <__main__.MyBlock_dense_norm_relu_drop_relu object at 0x7f8811421c18>>: ValueError: Failed to parse source code of <bound method MyBlock_dense_norm_relu_drop_relu.call of <__main__.MyBlock_dense_norm_relu_drop_relu object at 0x7f8811421c18>>, which Python reported as:\n",
      "    def call(self, inputs):\n",
      "        h1 = self.dense(inputs)\n",
      "        h1 = self.batch_norm(h1) \n",
      "#         h1 = tf.nn.relu(h1)\n",
      "        h1=  self.reul_func1(h1)\n",
      "        h1=self.drop_out(h1)\n",
      "#         h1 = tf.nn.relu(h1)\n",
      "        h1=  self.reul_func2(h1)\n",
      "        return h1\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "W0715 07:25:54.445928 140227208578816 ag_logging.py:145] Entity <bound method MyBlock_dense_norm_relu_drop_relu.call of <__main__.MyBlock_dense_norm_relu_drop_relu object at 0x7f8811061710>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MyBlock_dense_norm_relu_drop_relu.call of <__main__.MyBlock_dense_norm_relu_drop_relu object at 0x7f8811061710>>: ValueError: Failed to parse source code of <bound method MyBlock_dense_norm_relu_drop_relu.call of <__main__.MyBlock_dense_norm_relu_drop_relu object at 0x7f8811061710>>, which Python reported as:\n",
      "    def call(self, inputs):\n",
      "        h1 = self.dense(inputs)\n",
      "        h1 = self.batch_norm(h1) \n",
      "#         h1 = tf.nn.relu(h1)\n",
      "        h1=  self.reul_func1(h1)\n",
      "        h1=self.drop_out(h1)\n",
      "#         h1 = tf.nn.relu(h1)\n",
      "        h1=  self.reul_func2(h1)\n",
      "        return h1\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MyBlock_dense_norm_relu_drop_relu.call of <__main__.MyBlock_dense_norm_relu_drop_relu object at 0x7f8811421c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MyBlock_dense_norm_relu_drop_relu.call of <__main__.MyBlock_dense_norm_relu_drop_relu object at 0x7f8811421c18>>: ValueError: Failed to parse source code of <bound method MyBlock_dense_norm_relu_drop_relu.call of <__main__.MyBlock_dense_norm_relu_drop_relu object at 0x7f8811421c18>>, which Python reported as:\n",
      "    def call(self, inputs):\n",
      "        h1 = self.dense(inputs)\n",
      "        h1 = self.batch_norm(h1) \n",
      "#         h1 = tf.nn.relu(h1)\n",
      "        h1=  self.reul_func1(h1)\n",
      "        h1=self.drop_out(h1)\n",
      "#         h1 = tf.nn.relu(h1)\n",
      "        h1=  self.reul_func2(h1)\n",
      "        return h1\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MyBlock_dense_norm_relu_drop_relu.call of <__main__.MyBlock_dense_norm_relu_drop_relu object at 0x7f8811061710>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MyBlock_dense_norm_relu_drop_relu.call of <__main__.MyBlock_dense_norm_relu_drop_relu object at 0x7f8811061710>>: ValueError: Failed to parse source code of <bound method MyBlock_dense_norm_relu_drop_relu.call of <__main__.MyBlock_dense_norm_relu_drop_relu object at 0x7f8811061710>>, which Python reported as:\n",
      "    def call(self, inputs):\n",
      "        h1 = self.dense(inputs)\n",
      "        h1 = self.batch_norm(h1) \n",
      "#         h1 = tf.nn.relu(h1)\n",
      "        h1=  self.reul_func1(h1)\n",
      "        h1=self.drop_out(h1)\n",
      "#         h1 = tf.nn.relu(h1)\n",
      "        h1=  self.reul_func2(h1)\n",
      "        return h1\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "4/4 [==============================] - 1s 322ms/step - loss: 0.7478 - accuracy: 0.4464 - val_loss: 0.6894 - val_accuracy: 0.7347\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 0s 113ms/step - loss: 0.5886 - accuracy: 0.7427 - val_loss: 1.0637 - val_accuracy: 0.7347\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 0s 109ms/step - loss: 0.5379 - accuracy: 0.7770 - val_loss: 1.6838 - val_accuracy: 0.7347\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 0s 109ms/step - loss: 0.4782 - accuracy: 0.8145 - val_loss: 2.0649 - val_accuracy: 0.7347\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 0.4359 - accuracy: 0.8519 - val_loss: 2.2083 - val_accuracy: 0.7347\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 0s 109ms/step - loss: 0.4087 - accuracy: 0.8924 - val_loss: 2.0953 - val_accuracy: 0.7347\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 0s 109ms/step - loss: 0.3818 - accuracy: 0.8862 - val_loss: 1.8860 - val_accuracy: 0.7347\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 0s 113ms/step - loss: 0.3580 - accuracy: 0.8909 - val_loss: 1.7979 - val_accuracy: 0.7347\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 0s 112ms/step - loss: 0.3384 - accuracy: 0.8831 - val_loss: 1.7098 - val_accuracy: 0.7347\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 0.3201 - accuracy: 0.8955 - val_loss: 1.6002 - val_accuracy: 0.7347\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 0.3031 - accuracy: 0.9267 - val_loss: 1.4092 - val_accuracy: 0.7347\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 0s 122ms/step - loss: 0.2876 - accuracy: 0.9377 - val_loss: 1.2076 - val_accuracy: 0.7347\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 0s 110ms/step - loss: 0.2738 - accuracy: 0.9595 - val_loss: 1.0894 - val_accuracy: 0.7347\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 0s 109ms/step - loss: 0.2601 - accuracy: 0.9579 - val_loss: 1.1074 - val_accuracy: 0.7347\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 0.2483 - accuracy: 0.9657 - val_loss: 0.9888 - val_accuracy: 0.7347\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 0.2361 - accuracy: 0.9610 - val_loss: 0.9909 - val_accuracy: 0.7347\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 0.2247 - accuracy: 0.9657 - val_loss: 0.8819 - val_accuracy: 0.7347\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 0.2138 - accuracy: 0.9641 - val_loss: 0.8577 - val_accuracy: 0.7347\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 0.2038 - accuracy: 0.9688 - val_loss: 0.7576 - val_accuracy: 0.7347\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 0s 110ms/step - loss: 0.1947 - accuracy: 0.9766 - val_loss: 0.7760 - val_accuracy: 0.7347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f88114212e8>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, validation_data=val_ds,epochs=20)##不关变量，似乎都是在上一次的基础上继续训练的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step - loss: 0.7242 - accuracy: 0.7705\n",
      "Accuracy 0.7704918\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
